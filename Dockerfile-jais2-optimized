# Optimized Dockerfile-jais2 for reduced memory usage and faster builds
FROM vllm/vllm-openai:latest

# Set environment variables for pip optimization
ENV PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONUNBUFFERED=1 \
    pip_cache_dir=/tmp/pip-cache

# Create pip cache directory with proper permissions
RUN mkdir -p ${pip_cache_dir} && chmod 755 ${pip_cache_dir}

# Install git and the full CUDA toolkit in a single layer with cleanup
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    cuda-toolkit-12-9 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* \
    && rm -rf /var/tmp/*

# Configure pip to use temporary cache for reduced memory
RUN mkdir -p ~/.pip && \
    echo "[global]" > ~/.pip/pip.conf && \
    echo "cache-dir = ${pip_cache_dir}" >> ~/.pip/pip.conf && \
    echo "no-cache-dir = false" >> ~/.pip/pip.conf && \
    echo "disable-pip-version-check = true" >> ~/.pip/pip.conf

# Uninstall the standard packages to ensure a clean state with cleanup
RUN pip uninstall -y transformers vllm && \
    pip cache purge

# --- Step 1: Install the vllm fork from its local cloned source ---
# Clone the vllm repository to a temporary directory with shallow history
RUN git clone --depth 1 --branch jais2 --single-branch \
    https://github.com/inceptionai-abudhabi/vllm.git /tmp/vllm-build

# Set the working directory to the cloned vllm repo
WORKDIR /tmp/vllm-build

# Install vllm in editable mode with optimized pip settings
RUN pip install --no-cache-dir --timeout 300 --retries 3 \
    --cache-dir ${pip_cache_dir} -e . && \
    pip cache purge && \
    rm -rf /tmp/pip-cache/*

# cleanup build artifacts
RUN find . -name "*.pyc" -delete && \
    find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true

# --- Step 2: Install the transformers fork ---
# Return to the root directory to avoid path issues
WORKDIR /

# Install the custom transformers fork AFTER vllm, with optimized settings
RUN pip install --no-cache-dir --timeout 300 --retries 3 \
    git+https://github.com/inceptionai-abudhabi/transformers.git && \
    pip cache purge

# --- Standard vLLM setup ---
# Create a standard mount point for the persistent volume
RUN mkdir -p /root/.cache/huggingface

# Copy our startup script into the container
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Set the script as the entrypoint
ENTRYPOINT ["/start.sh"]

# Expose the vLLM API port
EXPOSE 8000